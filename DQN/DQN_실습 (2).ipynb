{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 시니어"
      ],
      "metadata": {
        "id": "dA0JjKWt18mp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- DQN 코드 작성 (전에 성능 나왔으면, DDQN 구현)  \n",
        "- 먼저 (5,5)와 같이 작은 사이즈의 환경에서 시작  \n",
        "- 매직넘버 사용 지양  \n",
        "\n"
      ],
      "metadata": {
        "id": "-UEg2buS1_vn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---  \n"
      ],
      "metadata": {
        "id": "2HJ1koTm3qwJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. **작은 사이즈, 적은 지뢰 밀도** 환경에서 학습되는지 확인  "
      ],
      "metadata": {
        "id": "oe65yDZ_3sHs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 시니어용 세팅"
      ],
      "metadata": {
        "id": "hXTJe-_B4vuL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EvTOcDT8z1B2",
        "outputId": "6ab9867e-617b-428e-edf9-83bbfbbb6c42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'DQN_minesweeper'...\n",
            "remote: Enumerating objects: 9824, done.\u001b[K\n",
            "remote: Counting objects: 100% (15/15), done.\u001b[K\n",
            "remote: Compressing objects: 100% (15/15), done.\u001b[K\n",
            "remote: Total 9824 (delta 8), reused 0 (delta 0), pack-reused 9809 (from 1)\u001b[K\n",
            "Receiving objects: 100% (9824/9824), 69.07 MiB | 9.40 MiB/s, done.\n",
            "Resolving deltas: 100% (2658/2658), done.\n",
            "Updating files: 100% (173/173), done.\n"
          ]
        }
      ],
      "source": [
        "! git clone \"https://github.com/KanghwaSisters/DQN_minesweeper.git\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('/content/DQN_minesweeper/codes/Environment')"
      ],
      "metadata": {
        "id": "p9gcnuUSr6be"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! python reward5.py\n",
        "from reward5 import *"
      ],
      "metadata": {
        "id": "yBP_YdvWsE3x"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = MinesweeperEnv(map_size=(5,5),\n",
        "                     n_mines=5,\n",
        "                     rewards={'win':1, 'lose':-1, 'progress':0.3, 'guess':0.3, 'no_progress' : -1},\n",
        "                     dones={'win':True, 'lose':True, 'progress':False, 'guess':False, 'no_progress' : False},\n",
        "                     dim2=False)"
      ],
      "metadata": {
        "id": "DNnwj-jO23E4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env.state"
      ],
      "metadata": {
        "id": "k6xQkXKBeazM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6a33df8-0249-49e0-d056-f21461614785"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.]],\n",
              "\n",
              "       [[1., 1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1., 1.]]])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.seed_mines()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cxgq1pZphuaV",
        "outputId": "6a528352-1686-4863-bbbf-7ffa3758cceb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 'M', 0, 0, 0],\n",
              "       [0, 'M', 0, 0, 'M'],\n",
              "       [0, 0, 'M', 0, 0],\n",
              "       [0, 'M', 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0]], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.board"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfKiwGzvidHU",
        "outputId": "097552f8-f6d6-4b1a-d926-977bb94fae67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[1, 1, 1, 1, 1],\n",
              "        [1, 1, 1, 1, 1],\n",
              "        [1, 1, 1, 1, 1],\n",
              "        [1, 1, 1, 1, 1],\n",
              "        [1, 1, 1, 1, 1]],\n",
              "\n",
              "       [[2, 'M', 3, 1, 0],\n",
              "        [2, 'M', 'M', 1, 0],\n",
              "        [2, 3, 2, 1, 0],\n",
              "        ['M', 2, 0, 0, 0],\n",
              "        ['M', 2, 0, 0, 0]]], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.step(4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-djYGG8il4y",
        "outputId": "e93984e1-62c9-408a-eae1-67a3271fb71e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[[0., 0., 0., 0., 1.],\n",
              "         [0., 0., 0., 0., 1.],\n",
              "         [0., 0., 0., 0., 1.],\n",
              "         [0., 0., 1., 1., 1.],\n",
              "         [0., 0., 1., 1., 1.]],\n",
              " \n",
              "        [[0., 0., 0., 1., 0.],\n",
              "         [0., 0., 0., 1., 0.],\n",
              "         [0., 0., 0., 1., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.]],\n",
              " \n",
              "        [[0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 1., 0., 0.],\n",
              "         [0., 1., 0., 0., 0.],\n",
              "         [0., 1., 0., 0., 0.]],\n",
              " \n",
              "        [[0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 1., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.]],\n",
              " \n",
              "        [[0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.]],\n",
              " \n",
              "        [[0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.]],\n",
              " \n",
              "        [[0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.]],\n",
              " \n",
              "        [[0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.]],\n",
              " \n",
              "        [[0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.]],\n",
              " \n",
              "        [[0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.]],\n",
              " \n",
              "        [[1., 1., 1., 0., 0.],\n",
              "         [1., 1., 1., 0., 0.],\n",
              "         [1., 0., 0., 0., 0.],\n",
              "         [1., 0., 0., 0., 0.],\n",
              "         [1., 0., 0., 0., 0.]]]),\n",
              " 0.3,\n",
              " False)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.step(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8u9GIxsiyko",
        "outputId": "32b54fda-8401-4cdc-b887-3b0c080fb041"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[[0., 0., 0., 0., 1.],\n",
              "         [0., 0., 0., 0., 1.],\n",
              "         [0., 0., 0., 0., 1.],\n",
              "         [0., 0., 1., 1., 1.],\n",
              "         [0., 0., 1., 1., 1.]],\n",
              " \n",
              "        [[0., 0., 0., 1., 0.],\n",
              "         [0., 0., 0., 1., 0.],\n",
              "         [0., 0., 0., 1., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.]],\n",
              " \n",
              "        [[0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 1., 0., 0.],\n",
              "         [0., 1., 0., 0., 0.],\n",
              "         [0., 1., 0., 0., 0.]],\n",
              " \n",
              "        [[0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 1., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.]],\n",
              " \n",
              "        [[0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.]],\n",
              " \n",
              "        [[0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.]],\n",
              " \n",
              "        [[0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.]],\n",
              " \n",
              "        [[0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.]],\n",
              " \n",
              "        [[0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.]],\n",
              " \n",
              "        [[0., 1., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.]],\n",
              " \n",
              "        [[1., 0., 1., 0., 0.],\n",
              "         [1., 1., 1., 0., 0.],\n",
              "         [1., 0., 0., 0., 0.],\n",
              "         [1., 0., 0., 0., 0.],\n",
              "         [1., 0., 0., 0., 0.]]]),\n",
              " -1,\n",
              " True)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.reset()"
      ],
      "metadata": {
        "id": "lOZPSPnUi3kv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env.state"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zl_SF-ZJjk6a",
        "outputId": "d48ae7c8-aa16-4c13-ba6b-5d4a2bbd756c",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.]],\n",
              "\n",
              "       [[1., 1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1., 1.]]])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NN"
      ],
      "metadata": {
        "id": "bVNFt55wkadn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "B9Cp2jFyke2H"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNet(nn.Module):\n",
        "\n",
        "    def __init__(self, output):\n",
        "\n",
        "        super(NeuralNet, self).__init__()\n",
        "\n",
        "        self.drop_prob = 0.25\n",
        "\n",
        "        self.conv1 = nn.Conv2d(11, 64, kernel_size = 3, stride = 1, padding = 1)\n",
        "        self.batchnorm1 = nn.BatchNorm2d(64)\n",
        "        self.conv2 = nn.Conv2d(64, 64, kernel_size = 3, stride = 1, padding = 1)\n",
        "        self.batchnorm2 = nn.BatchNorm2d(64)\n",
        "        self.conv3 = nn.Conv2d(64, 64, kernel_size = 3, stride = 1, padding = 1)\n",
        "        self.batchnorm3 = nn.BatchNorm2d(64)\n",
        "        self.dropout = nn.Dropout(p = self.drop_prob)\n",
        "        self.fc1 = nn.Linear(64*5*5, 128)\n",
        "        self.fc2 = nn.Linear(128, output)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(self.batchnorm1(x))\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(self.batchnorm2(x))\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        x = F.relu(self.batchnorm3(x))\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = x.view(-1, 64*5*5)\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "vpMpfPRlkMDQ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Agent"
      ],
      "metadata": {
        "id": "bHkk3h2VkKHY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "nSyDr6aGlHY2"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "MEMORY_SIZE = 2000\n",
        "MEMORY_SIZE_MIN = 100\n",
        "\n",
        "EPSILON = 0.99\n",
        "EPSILON_DECAY = 0.999\n",
        "EPSILON_MIN = 0.01\n",
        "DISCOUNT_FACTOR = 0.1\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "LEARNING_RATE = 0.001\n",
        "LEARNING_RATE_DECAY = 0.99975\n",
        "LEARN_EPOCH = 500\n",
        "\n",
        "UPDATE_TIME = 5"
      ],
      "metadata": {
        "id": "6UH1_6UakZka"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DQN"
      ],
      "metadata": {
        "id": "CdGNir8dGn0z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent:\n",
        "\n",
        "    def __init__(self, action_size):\n",
        "\n",
        "        self.action_size = action_size\n",
        "\n",
        "        self.batch_size = BATCH_SIZE\n",
        "        self.memory_size = MEMORY_SIZE\n",
        "        self.memory_size_min = MEMORY_SIZE_MIN\n",
        "        self.memory = deque(maxlen = self.memory_size)\n",
        "\n",
        "        self.epsilon = EPSILON\n",
        "        self.epsilon_decay = EPSILON_DECAY\n",
        "        self.epsilon_min = EPSILON_MIN\n",
        "        self.discount_factor = DISCOUNT_FACTOR\n",
        "\n",
        "        self.device = DEVICE\n",
        "\n",
        "        self.learning_rate = LEARNING_RATE\n",
        "        self.learning_rate_decay = LEARNING_RATE_DECAY\n",
        "        self.learn_epoch = LEARN_EPOCH\n",
        "        self.loss = nn.MSELoss()\n",
        "        self.loss_list = []\n",
        "        self.model = NeuralNet(self.action_size).to(self.device)\n",
        "        self.target_model = NeuralNet(self.action_size).to(self.device)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr = self.learning_rate, eps = 1e-4)\n",
        "        self.lr_scheduler = lr_scheduler.CyclicLR(self.optimizer, base_lr=0.00005,\n",
        "                                              step_size_up=5, max_lr=0.01,\n",
        "                                              gamma=0.5, mode='exp_range')\n",
        "\n",
        "        self.update_time = UPDATE_TIME\n",
        "        self.update_target_model()\n",
        "\n",
        "    def update_target_model(self):\n",
        "        self.target_model.load_state_dict(self.model.state_dict())\n",
        "\n",
        "    def get_action(self, state):\n",
        "        random_prob = np.random.rand()\n",
        "        if random_prob <= self.epsilon:\n",
        "            action_idx = np.random.choice(range(self.action_size))\n",
        "        else:\n",
        "            self.model.eval()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                state = torch.tensor(state, dtype=torch.float32).to(self.device).unsqueeze(0) ##\n",
        "                q_value = self.model(state)\n",
        "                _, action_idx = torch.max(q_value, dim=1)\n",
        "                action_idx = action_idx.item()\n",
        "\n",
        "        return action_idx\n",
        "\n",
        "    def append_sample(self, state, action, next_state, reward, done):\n",
        "        self.memory.append((state, action, next_state, reward, done))\n",
        "\n",
        "    def train_model(self):\n",
        "\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "        mini_batch = random.sample(self.memory, self.batch_size)\n",
        "        states, actions, next_states, rewards, dones = zip(*mini_batch)\n",
        "\n",
        "        states = torch.tensor(states, dtype = torch.float32).to(self.device)#\n",
        "        actions = torch.tensor(actions, dtype = torch.int64).to(self.device)\n",
        "        next_states = torch.tensor(next_states, dtype = torch.float32).to(self.device)#\n",
        "        rewards = torch.tensor(rewards, dtype = torch.float32).to(self.device).reshape(-1, 1)\n",
        "        dones = torch.tensor(dones, dtype = torch.int64).to(self.device).reshape(-1, 1)\n",
        "\n",
        "\n",
        "        self.model.train()\n",
        "        self.target_model.eval()\n",
        "\n",
        "        predicts = self.model(states)\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "            target_predict, _ = torch.max(self.target_model(next_states), dim = 1)\n",
        "            target_pred = target_predict.reshape(-1, 1)\n",
        "\n",
        "        targets = rewards + (1 - dones) * self.discount_factor * target_pred\n",
        "        targets = targets.flatten()\n",
        "\n",
        "        target_q_values = copy.deepcopy(predicts.detach())\n",
        "        target_q_values[range(BATCH_SIZE), actions] = targets\n",
        "\n",
        "        loss = self.loss(predicts, target_q_values)\n",
        "        self.loss_list.append(loss.item())\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n"
      ],
      "metadata": {
        "id": "lOZ2jjSkkpPU"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DDQN"
      ],
      "metadata": {
        "id": "PP1-G4bqGpjr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent:\n",
        "\n",
        "    def __init__(self, action_size):\n",
        "\n",
        "        self.action_size = action_size\n",
        "\n",
        "        self.batch_size = BATCH_SIZE\n",
        "        self.memory_size = MEMORY_SIZE\n",
        "        self.memory_size_min = MEMORY_SIZE_MIN\n",
        "        self.memory = deque(maxlen = self.memory_size)\n",
        "\n",
        "        self.epsilon = EPSILON\n",
        "        self.epsilon_decay = EPSILON_DECAY\n",
        "        self.epsilon_min = EPSILON_MIN\n",
        "        self.discount_factor = DISCOUNT_FACTOR\n",
        "\n",
        "        self.device = DEVICE\n",
        "\n",
        "        self.learning_rate = LEARNING_RATE\n",
        "        self.learning_rate_decay = LEARNING_RATE_DECAY\n",
        "        self.learn_epoch = LEARN_EPOCH\n",
        "        self.loss = nn.MSELoss()\n",
        "        self.loss_list = []\n",
        "        self.model = NeuralNet(self.action_size).to(self.device)\n",
        "        self.target_model = NeuralNet(self.action_size).to(self.device)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr = self.learning_rate, eps = 1e-4)\n",
        "        self.lr_scheduler = lr_scheduler.CyclicLR(self.optimizer, base_lr=0.00005,\n",
        "                                              step_size_up=5, max_lr=0.01,\n",
        "                                              gamma=0.5, mode='exp_range')\n",
        "\n",
        "        self.update_time = UPDATE_TIME\n",
        "        self.update_target_model()\n",
        "\n",
        "    def update_target_model(self):\n",
        "        self.target_model.load_state_dict(self.model.state_dict())\n",
        "\n",
        "    def get_action(self, state):\n",
        "        random_prob = np.random.rand()\n",
        "        if random_prob <= self.epsilon:\n",
        "            action_idx = np.random.choice(range(self.action_size))\n",
        "        else:\n",
        "            self.model.eval()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                state = torch.tensor(state, dtype=torch.float32).to(self.device).unsqueeze(0) ##\n",
        "                q_value = self.model(state)\n",
        "                _, action_idx = torch.max(q_value, dim=1)\n",
        "                action_idx = action_idx.item()\n",
        "\n",
        "        return action_idx\n",
        "\n",
        "    def append_sample(self, state, action, next_state, reward, done):\n",
        "        # done: 게임이 끝났으면 True\n",
        "        # 이때 reward를 다르게 줌\n",
        "        self.memory.append((state, action, next_state, reward, done))\n",
        "\n",
        "    def train_model(self):\n",
        "\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "        mini_batch = random.sample(self.memory, self.batch_size)\n",
        "        states, actions, next_states, rewards, dones = zip(*mini_batch)\n",
        "\n",
        "        states = torch.tensor(states, dtype = torch.float32).to(self.device)#\n",
        "        actions = torch.tensor(actions, dtype = torch.int64).to(self.device)\n",
        "        next_states = torch.tensor(next_states, dtype = torch.float32).to(self.device)#\n",
        "        rewards = torch.tensor(rewards, dtype = torch.float32).to(self.device).reshape(-1, 1)\n",
        "        dones = torch.tensor(dones, dtype = torch.int64).to(self.device).reshape(-1, 1)\n",
        "\n",
        "\n",
        "        self.model.train()\n",
        "        self.target_model.eval()\n",
        "\n",
        "        # 현재 상태에 대한 action별 큐함수(인공신경망의 예측)\n",
        "        predicts = self.model(states)\n",
        "\n",
        "        with torch.no_grad():\n",
        "        # DDQN: 현재 모델로 다음 상태에서 최대 Q-value의 행동 선택\n",
        "          next_action_indices = torch.argmax(self.model(next_states), dim=1).unsqueeze(1)\n",
        "\n",
        "          # 타깃 모델을 사용해 다음 상태에서 선택한 행동에 대한 Q-value 추출\n",
        "          target_q_values = self.target_model(next_states).gather(1, next_action_indices)\n",
        "\n",
        "          # 벨만 최적 방정식으로 타겟 Q-value 계산\n",
        "          targets = rewards + (1 - dones) * self.discount_factor * target_q_values\n",
        "\n",
        "        # 선택한 행동에 대한 Q-value만 업데이트\n",
        "        target_q_values = copy.deepcopy(predicts.detach())\n",
        "        target_q_values[range(self.batch_size), actions] = targets.flatten()\n",
        "\n",
        "        # 손실 계산\n",
        "        loss = self.loss(predicts, target_q_values)\n",
        "        self.loss_list.append(loss.item())\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n"
      ],
      "metadata": {
        "id": "qrMF1Cw9rqUn"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DQN Main"
      ],
      "metadata": {
        "id": "chWSUS1jlUlE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "env = MinesweeperEnv(map_size=(5,5),\n",
        "                     n_mines=5,\n",
        "                     rewards={'win': 1, 'lose': -1, 'progress': 0.3, 'guess': 0.3, 'no_progress': -1},\n",
        "                     dones={'win': True, 'lose': True, 'progress': False, 'guess': False, 'no_progress': False},\n",
        "                     dim2=False)\n",
        "\n",
        "# 에이전트 초기화\n",
        "agent = Agent(action_size=25)\n",
        "\n",
        "# 학습 설정\n",
        "EPISODES = 2000\n",
        "episodes, scores, score_avg, total_action, length_memory, win_list = [], [], [], [], [], []\n",
        "\n",
        "for epi in range(EPISODES):\n",
        "    done = False\n",
        "    score = 0\n",
        "    total_action_epi = []\n",
        "\n",
        "    # 게임 초기화\n",
        "    env.reset()\n",
        "    state = env.state\n",
        "\n",
        "    while not done:\n",
        "        # 현재 상태에 대한 행동 선택\n",
        "        action = agent.get_action(state)\n",
        "        total_action_epi.append(action)\n",
        "\n",
        "        # 선택한 행동에 따라 다음 상태, 보상 및 종료 여부 얻기\n",
        "        next_state, reward, done = env.step(action)\n",
        "        score += reward\n",
        "        agent.append_sample(state, action, next_state, reward, done)\n",
        "\n",
        "        # 메모리가 충분히 쌓인 후 훈련 시작\n",
        "        if len(agent.memory) > MEMORY_SIZE_MIN:\n",
        "            agent.train_model()\n",
        "            if done:\n",
        "                agent.lr_scheduler.step()\n",
        "\n",
        "        # 다음 상태를 현재 상태로 업데이트\n",
        "        state = next_state\n",
        "\n",
        "        # 종료 조건 처리\n",
        "        if done:\n",
        "            win_list.append(1 if reward == 1 else 0)\n",
        "\n",
        "            # 주기적으로 타깃 모델 업데이트\n",
        "            if (epi % UPDATE_TIME) == 0:\n",
        "                agent.update_target_model()\n",
        "\n",
        "            # 에피소드 결과 저장\n",
        "            total_action.append(len(total_action_epi))\n",
        "            episodes.append(epi)\n",
        "            scores.append(score)\n",
        "            score_avg.append(np.mean(scores[-100:]))\n",
        "            length_memory.append(len(agent.memory))\n",
        "\n",
        "            # 100번째 에피소드마다 진행상황 출력\n",
        "            if (epi % 100) == 0:\n",
        "                win_rate = np.mean(win_list[-100:]) if len(win_list) >= 100 else np.mean(win_list)\n",
        "                loss_avg = np.mean(agent.loss_list[-100:]) if len(agent.loss_list) >= 100 else np.mean(agent.loss_list)\n",
        "                action_avg = np.mean(total_action[-100:]) if len(total_action) >= 100 else np.mean(total_action)\n",
        "                print(f\"Episode: {epi} | Score: {np.median(scores[-100:]):.5f} | Epsilon: {agent.epsilon:.2f} | Win Rate: {win_rate}, Loss avg: {loss_avg:.5f} | Action avg: {action_avg}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFspPI1OvZ7K",
        "outputId": "603eaff0-365d-457d-f5c7-e8b6f890678f"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 0 | Score: -2.10000 | Epsilon: 1.00 | Win Rate: 0.0, Loss avg: nan | Action avg: 6.0\n",
            "Episode: 100 | Score: -0.70000 | Epsilon: 0.10 | Win Rate: 0.02, Loss avg: 0.00543 | Action avg: 7.5\n",
            "Episode: 200 | Score: -0.70000 | Epsilon: 0.10 | Win Rate: 0.01, Loss avg: 0.00603 | Action avg: 8.71\n",
            "Episode: 300 | Score: -0.40000 | Epsilon: 0.10 | Win Rate: 0.01, Loss avg: 0.00617 | Action avg: 8.36\n",
            "Episode: 400 | Score: -0.40000 | Epsilon: 0.10 | Win Rate: 0.01, Loss avg: 0.00882 | Action avg: 6.7\n",
            "Episode: 500 | Score: -0.40000 | Epsilon: 0.10 | Win Rate: 0.03, Loss avg: 0.00814 | Action avg: 7.16\n",
            "Episode: 600 | Score: -0.40000 | Epsilon: 0.10 | Win Rate: 0.0, Loss avg: 0.01243 | Action avg: 6.3\n",
            "Episode: 700 | Score: -0.40000 | Epsilon: 0.10 | Win Rate: 0.0, Loss avg: 0.01147 | Action avg: 6.03\n",
            "Episode: 800 | Score: -0.40000 | Epsilon: 0.10 | Win Rate: 0.02, Loss avg: 0.01287 | Action avg: 5.35\n",
            "Episode: 900 | Score: -0.40000 | Epsilon: 0.10 | Win Rate: 0.02, Loss avg: 0.01120 | Action avg: 5.93\n",
            "Episode: 1000 | Score: -0.10000 | Epsilon: 0.10 | Win Rate: 0.01, Loss avg: 0.00938 | Action avg: 6.0\n",
            "Episode: 1100 | Score: -0.40000 | Epsilon: 0.10 | Win Rate: 0.01, Loss avg: 0.00778 | Action avg: 7.04\n",
            "Episode: 1200 | Score: -0.40000 | Epsilon: 0.10 | Win Rate: 0.02, Loss avg: 0.01198 | Action avg: 5.82\n",
            "Episode: 1300 | Score: -0.40000 | Epsilon: 0.10 | Win Rate: 0.02, Loss avg: 0.01025 | Action avg: 5.13\n",
            "Episode: 1400 | Score: -0.40000 | Epsilon: 0.10 | Win Rate: 0.03, Loss avg: 0.01092 | Action avg: 6.0\n",
            "Episode: 1500 | Score: -0.40000 | Epsilon: 0.10 | Win Rate: 0.03, Loss avg: 0.00996 | Action avg: 5.45\n",
            "Episode: 1600 | Score: -0.40000 | Epsilon: 0.10 | Win Rate: 0.05, Loss avg: 0.01097 | Action avg: 6.39\n",
            "Episode: 1700 | Score: -0.10000 | Epsilon: 0.10 | Win Rate: 0.05, Loss avg: 0.00855 | Action avg: 5.89\n",
            "Episode: 1800 | Score: -0.40000 | Epsilon: 0.10 | Win Rate: 0.0, Loss avg: 0.00830 | Action avg: 5.13\n",
            "Episode: 1900 | Score: -0.10000 | Epsilon: 0.10 | Win Rate: 0.0, Loss avg: 0.01024 | Action avg: 5.71\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DDQN Main"
      ],
      "metadata": {
        "id": "RFlL6AIzGseu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "env = MinesweeperEnv(map_size=(5,5),\n",
        "                     n_mines=5,\n",
        "                     rewards={'win': 1, 'lose': -1, 'progress': 0.3, 'guess': 0.3, 'no_progress': -1},\n",
        "                     dones={'win': True, 'lose': True, 'progress': False, 'guess': False, 'no_progress': False},\n",
        "                     dim2=False)\n",
        "\n",
        "# 에이전트 초기화\n",
        "agent = Agent(action_size=25)\n",
        "\n",
        "# 학습 설정\n",
        "EPISODES = 2000\n",
        "episodes, scores, score_avg, total_action, length_memory, win_list = [], [], [], [], [], []\n",
        "\n",
        "for epi in range(EPISODES):\n",
        "    done = False\n",
        "    score = 0\n",
        "    total_action_epi = []\n",
        "\n",
        "    # 게임 초기화\n",
        "    env.reset()\n",
        "    state = env.state\n",
        "\n",
        "    while not done:\n",
        "        # 현재 상태에 대한 행동 선택\n",
        "        action = agent.get_action(state)\n",
        "        total_action_epi.append(action)\n",
        "\n",
        "        # 선택한 행동에 따라 다음 상태, 보상 및 종료 여부 얻기\n",
        "        next_state, reward, done = env.step(action)\n",
        "        score += reward\n",
        "        agent.append_sample(state, action, next_state, reward, done)\n",
        "\n",
        "        # 메모리가 충분히 쌓인 후 훈련 시작\n",
        "        if len(agent.memory) > MEMORY_SIZE_MIN:\n",
        "            agent.train_model()\n",
        "            if done:\n",
        "                agent.lr_scheduler.step()\n",
        "\n",
        "        # 다음 상태를 현재 상태로 업데이트\n",
        "        state = next_state\n",
        "\n",
        "        # 종료 조건 처리\n",
        "        if done:\n",
        "            win_list.append(1 if reward == 1 else 0)\n",
        "\n",
        "            # 주기적으로 타깃 모델 업데이트\n",
        "            if (epi % UPDATE_TIME) == 0:\n",
        "                agent.update_target_model()\n",
        "\n",
        "            # 에피소드 결과 저장\n",
        "            total_action.append(len(total_action_epi))\n",
        "            episodes.append(epi)\n",
        "            scores.append(score)\n",
        "            score_avg.append(np.mean(scores[-100:]))\n",
        "            length_memory.append(len(agent.memory))\n",
        "\n",
        "            # 100번째 에피소드마다 진행상황 출력\n",
        "            if (epi % 100) == 0:\n",
        "                win_rate = np.mean(win_list[-100:]) if len(win_list) >= 100 else np.mean(win_list)\n",
        "                loss_avg = np.mean(agent.loss_list[-100:]) if len(agent.loss_list) >= 100 else np.mean(agent.loss_list)\n",
        "                action_avg = np.mean(total_action[-100:]) if len(total_action) >= 100 else np.mean(total_action)\n",
        "                print(f\"Episode: {epi} | Score: {np.median(scores[-100:]):.5f} | Epsilon: {agent.epsilon:.2f} | Win Rate: {win_rate}, Loss avg: {loss_avg:.5f} | Action avg: {action_avg}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c37e1d1-990a-4703-b8ff-4ddd3821d02e",
        "id": "-c6wl1WnsCNZ"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 0 | Score: -4.10000 | Epsilon: 0.99 | Win Rate: 0.0, Loss avg: nan | Action avg: 8.0\n",
            "Episode: 100 | Score: -0.70000 | Epsilon: 0.58 | Win Rate: 0.02, Loss avg: 0.00485 | Action avg: 6.32\n",
            "Episode: 200 | Score: -0.70000 | Epsilon: 0.27 | Win Rate: 0.01, Loss avg: 0.00460 | Action avg: 7.73\n",
            "Episode: 300 | Score: -0.40000 | Epsilon: 0.14 | Win Rate: 0.01, Loss avg: 0.00430 | Action avg: 6.42\n",
            "Episode: 400 | Score: -0.40000 | Epsilon: 0.08 | Win Rate: 0.01, Loss avg: 0.00468 | Action avg: 5.68\n",
            "Episode: 500 | Score: -0.40000 | Epsilon: 0.04 | Win Rate: 0.01, Loss avg: 0.00459 | Action avg: 7.26\n",
            "Episode: 600 | Score: -0.40000 | Epsilon: 0.02 | Win Rate: 0.02, Loss avg: 0.00469 | Action avg: 5.9\n",
            "Episode: 700 | Score: -0.10000 | Epsilon: 0.01 | Win Rate: 0.05, Loss avg: 0.00403 | Action avg: 9.12\n",
            "Episode: 800 | Score: -0.40000 | Epsilon: 0.01 | Win Rate: 0.02, Loss avg: 0.00366 | Action avg: 10.53\n",
            "Episode: 900 | Score: -0.10000 | Epsilon: 0.01 | Win Rate: 0.03, Loss avg: 0.00378 | Action avg: 6.58\n",
            "Episode: 1000 | Score: -0.40000 | Epsilon: 0.01 | Win Rate: 0.02, Loss avg: 0.00318 | Action avg: 12.76\n",
            "Episode: 1100 | Score: -0.25000 | Epsilon: 0.01 | Win Rate: 0.02, Loss avg: 0.00291 | Action avg: 9.69\n",
            "Episode: 1200 | Score: -0.10000 | Epsilon: 0.01 | Win Rate: 0.02, Loss avg: 0.00361 | Action avg: 7.32\n",
            "Episode: 1300 | Score: -0.10000 | Epsilon: 0.01 | Win Rate: 0.02, Loss avg: 0.00365 | Action avg: 7.42\n",
            "Episode: 1400 | Score: -0.10000 | Epsilon: 0.01 | Win Rate: 0.03, Loss avg: 0.00394 | Action avg: 8.05\n",
            "Episode: 1500 | Score: -0.10000 | Epsilon: 0.01 | Win Rate: 0.02, Loss avg: 0.00411 | Action avg: 7.22\n",
            "Episode: 1600 | Score: -0.10000 | Epsilon: 0.01 | Win Rate: 0.03, Loss avg: 0.00394 | Action avg: 8.44\n",
            "Episode: 1700 | Score: -0.10000 | Epsilon: 0.01 | Win Rate: 0.03, Loss avg: 0.00392 | Action avg: 7.54\n",
            "Episode: 1800 | Score: -0.10000 | Epsilon: 0.01 | Win Rate: 0.02, Loss avg: 0.00399 | Action avg: 7.1\n",
            "Episode: 1900 | Score: -0.10000 | Epsilon: 0.01 | Win Rate: 0.04, Loss avg: 0.00370 | Action avg: 8.98\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 주니어"
      ],
      "metadata": {
        "id": "ByorAZi-42Eu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. 레버 환경 만들기  \n",
        "2. 카트폴 예제 -- 속도 개선, 학습 가능하게 만들기  \n",
        "3. 매직 넘버 줄이기\n",
        "4. 중간 중간 디버깅하면서 가기(구현 단위 별로 테스트)"
      ],
      "metadata": {
        "id": "7Jfvc4lq5wgz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. 레버 환경 만들기"
      ],
      "metadata": {
        "id": "J8NzcPDf66x4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3개의 레버가 있다.  \n",
        "- 1번 레버를 당겼을 때 바나나가 떨어질 확률이 0.7, 아무 것도 떨어지지 않을 확률이 0.3  \n",
        "- 2번 레버를 당겼을 때 바나나가 떨어질 확률이 0.1, 상한 바나나 껍질만 떨어질 확률이 0.8, 아무 것도 떨어지지 않을 확률이 0.1  \n",
        "- 3번 레버를 당겼을 때 상한 바나나 껍질만 떨어질 확률이 0.5, 아무 것도 떨어지지 않을 확률이 0.5  \n",
        "\n"
      ],
      "metadata": {
        "id": "YIBM-0JC6-b-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. 상태를 표현하는 방법   \n",
        "2. 보상 구조 설계  \n",
        "3. 한 번 행동 한 후 초기화하는 걸 전제    \n",
        "4. 환경 로직 설계"
      ],
      "metadata": {
        "id": "9-Es7EwE8tqc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LeverEnv:\n",
        "    def __init__(self, **):\n",
        "        pass\n",
        "\n",
        "    def step(self, action):\n",
        "        pass\n",
        "\n",
        "    def reset(self):\n",
        "        pass\n",
        "\n",
        "    def render(self):\n",
        "        pass"
      ],
      "metadata": {
        "id": "xr5hwLke43Wr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 과제\n",
        "1. 완성해오기(설명가능한 상태로 ipynb 파일 구성)    \n",
        "2. 알파제로 구조 공부해오기(전원)  \n",
        "3. 자기 코드 규칙 정리해오기"
      ],
      "metadata": {
        "id": "WyHQdX1W91iN"
      }
    }
  ]
}